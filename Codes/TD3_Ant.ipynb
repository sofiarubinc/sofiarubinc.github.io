{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_Ant.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y2nGdtlKVydr",
        "Jb7TTaHxWbQD",
        "HRDDce8FXef7",
        "ka-ZRtQvjBex",
        "gGuKmH_ijf7U",
        "Hjwf2HCol3XP",
        "kop-C96Aml8O",
        "qEAzOd47mv1Z",
        "5YdPG4HXnNsh",
        "HWEgDAQxnbem",
        "ZI60VN2Unklh",
        "QYOpCyiDnw7s",
        "xm-4b3p6rglE",
        "31n5eb03p-Fm",
        "q9gsjvtPqLgT",
        "wi6e2-_pu05e"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiarubinc/sofiarubinc.github.io/blob/main/Codes/TD3_Ant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Gradiente de política determinista profunda (TD3) de doble retardo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Instalación de los paquetes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a1c48446-3047-4d28-d335-5c36d5759aa5"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Paso 1: Inicializamos la memoria de la repetición de experiencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage)== self.max_size: ####OJO, en el vídeo ponía MAX_STORAGE!!!\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size = batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy = False))\n",
        "      batch_next_states.append(np.array(next_state, copy = False))\n",
        "      batch_actions.append(np.array(action, copy = False))\n",
        "      batch_rewards.append(np.array(reward, copy = False))\n",
        "      batch_dones.append(np.array(done, copy = False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Paso 2: Construimos una red neuronal para el **actor del modelo** y una red neuronal para el **actor del objetivo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7"
      },
      "source": [
        "\n",
        "## Paso 3: Construimos dos redes neuronales para los dos **críticos del modelo** y dos redes neuronales para los dos **críticos del objetivo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Definimos el primero de los Críticos como red neuronal profunda\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Definimos el segundo de los Críticos como red neuronal profunda\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Propagación hacia adelante del primero de los Críticos\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Propagación hacia adelante del segundo de los Críticos\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "  \n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Pasos 4 a 15: Proceso de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "source": [
        "# Selección del dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construir todo el proceso de entrenamiento en una clase\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clipping=0.5, policy_freq=2):\n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Paso 4: Tomamos una muestra de transiciones (s, s’, a, r) de la memoria.\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Paso 5: A partir del estado siguiente s', el Actor del Target ejecuta la siguiente acción a'.\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Paso 6: Añadimos ruido gaussiano a la siguiente acción a' y lo cortamos para tenerlo en el rango de valores aceptado por el entorno.\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device) \n",
        "      noise = noise.clamp(-noise_clipping, noise_clipping)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Paso 7: Los dos Críticos del Target toman un par (s’, a’) como entrada y devuelven dos Q-values Qt1(s’,a’) y Qt2(s’,a’) como salida.\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Paso 8: Nos quedamos con el mínimo de los dos Q-values: min(Qt1, Qt2). Representa el valor aproximado del estado siguiente.\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Paso 9: Obtenemos el target final de los dos Crítico del Modelo, que es: Qt = r + γ * min(Qt1, Qt2), donde γ es el factor de descuento.\n",
        "      target_Q = reward + ((1-done) * discount * target_Q).detach()\n",
        "\n",
        "      # Paso 10: Los dos Críticos del Modelo toman un par (s, a) como entrada y devuelven dos Q-values Q1(s,a) y Q2(s,a) como salida.\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Paso 11: Calculamos la pérdida procedente de los Crítico del Modelo: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Paso 12: Propagamos hacia atrás la pérdida del crítico y actualizamos los parámetros de los dos Crítico del Modelo con un SGD.\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Paso 13: Cada dos iteraciones, actualizamos nuestro modelo de Actor ejecutando el gradiente ascendente en la salida del primer modelo crítico.\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()##OJO ME DEJÉ EL LOSS\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Paso 14: Todavía cada dos iteraciones, actualizamos los pesos del Actor del Target usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "        # Paso 15: Todavía cada dos iteraciones, actualizamos los pesos del target del Crítico usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "  # Método para guardar el modelo entrenado\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n",
        "\n",
        "  # Método para cargar el modelo entrenado\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load(\"%s/%s_actor.pth\" % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load(\"%s/%s_critic.pth\" % (directory, filename)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## Hacemos una función que evalúa la política calculando su recompensa promedio durante 10 episodios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"-------------------------------------------------\")\n",
        "  print (\"Recompensa promedio en el paso de Evaluación: %f\" % (avg_reward))\n",
        "  print (\"-------------------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## Configuramos los parámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Nombre del entorno (puedes indicar cualquier entorno continuo que quieras probar aquí)\n",
        "seed = 0 # Valor de la semilla aleatoria\n",
        "start_timesteps = 1e4 # Número de of iteraciones/timesteps durante las cuales el modelo elige una acción al azar, y después de las cuales comienza a usar la red de políticas\n",
        "eval_freq = 5e3 # Con qué frecuencia se realiza el paso de evaluación (después de cuántos pasos timesteps)\n",
        "max_timesteps = 5e5 # Número total de iteraciones/timesteps\n",
        "save_models = True # Check Boolean para saber si guardar o no el modelo pre-entrenado\n",
        "expl_noise = 0.1 # Ruido de exploración: desviación estándar del ruido de exploración gaussiano\n",
        "batch_size = 100 # Tamaño del bloque\n",
        "discount = 0.99 # Factor de descuento gamma, utilizado en el cáclulo de la recompensa de descuento total\n",
        "tau = 0.005 # Ratio de actualización de la red de objetivos\n",
        "policy_noise = 0.2 # Desviación estándar del ruido gaussiano añadido a las acciones para fines de exploración\n",
        "noise_clip = 0.5 # Valor máximo de ruido gaussiano añadido a las acciones (política)\n",
        "policy_freq = 2 # Número de iteraciones a esperar antes de actualizar la red de políticas (actor modelo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## Creamos un nombre de archivo para los dos modelos guardados: los modelos Actor y Crítico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d8908f30-6b8f-4a05-b019-4ab490e8535f"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Configuración: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Configuración: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## Creamos una carpeta dentro de la cual se guardarán los modelos entrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## Creamos un entorno de `PyBullet`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9c64d020-63d4-4a87-fab6-6cdc86760437"
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## Establecemos las semillas y obtenemos la información necesaria sobre los estados y las acciones en el entorno elegido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj"
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## Creamos la red neronal de la política (el actor del modelo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg"
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## Creamos la memoria de la repetición de experiencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## Definimos una lista donde se guardaran los resultados de evaluación de los 10 episodios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "201cc044-bb28-4f0e-eb70-c9865d6ed86e"
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 9.804960\n",
            "-------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## Creamos un nuevo directorio de carpetas en el que se mostrarán los resultados finales (videos del agente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## Inicializamos las variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d0f6399-c976-4e7c-d34e-959bcd4b235d"
      },
      "source": [
        "# Iniciamos el bucle principal con un total de 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # Si el episodio ha terminado\n",
        "  if done:\n",
        "\n",
        "    # Si no estamos en la primera de las iteraciones, arrancamos el proceso de entrenar el modelo\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # Evaluamos el episodio y guardamos la política si han pasado las iteraciones necesarias\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # Cuando el entrenamiento de un episodio finaliza, reseteamos el entorno\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Configuramos el valor de done a False\n",
        "    done = False\n",
        "    \n",
        "    # Configuramos la recompensa y el timestep del episodio a cero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Antes de los 10000 timesteps, ejectuamos acciones aleatorias\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # Después de los 10000 timesteps, cambiamos al modelo\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # Si el valor de explore_noise no es 0, añadimos ruido a la acción y lo recortamos en el rango adecuado\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # El agente ejecuta una acción en el entorno y alcanza el siguiente estado y una recompensa\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # Comprobamos si el episodio ha terminado\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # Incrementamos la recompensa total\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # Almacenamos la nueva transición en la memoria de repetición de experiencias (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # Actualizamos el estado, el timestep del número de episodio, el total de timesteps y el número de pasos desde la última evaluación de la política\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# Añadimos la última actualización de la política a la lista de evaluaciones previa y guardamos nuestro modelo\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 500.07056060936213\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 489.2427992494504\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 496.4517711308622\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 479.36893811372073\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 520.1372282581459\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 87.937180\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 5333 Episode Num: 6 Reward: 160.93831699797437\n",
            "Total Timesteps: 6333 Episode Num: 7 Reward: 518.4860545731583\n",
            "Total Timesteps: 6790 Episode Num: 8 Reward: 209.77761294241634\n",
            "Total Timesteps: 7790 Episode Num: 9 Reward: 525.6336129662293\n",
            "Total Timesteps: 8790 Episode Num: 10 Reward: 485.33244607187055\n",
            "Total Timesteps: 9790 Episode Num: 11 Reward: 506.6846379008975\n",
            "Total Timesteps: 9813 Episode Num: 12 Reward: 6.524857094473553\n",
            "Total Timesteps: 10813 Episode Num: 13 Reward: 265.6708624060588\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 105.312366\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 11813 Episode Num: 14 Reward: 99.19534153318665\n",
            "Total Timesteps: 12813 Episode Num: 15 Reward: 107.12843555251214\n",
            "Total Timesteps: 13813 Episode Num: 16 Reward: 150.97741072777825\n",
            "Total Timesteps: 14813 Episode Num: 17 Reward: 88.65697622259776\n",
            "Total Timesteps: 15813 Episode Num: 18 Reward: 292.79460015708474\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 118.430765\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 16813 Episode Num: 19 Reward: 189.47731207157122\n",
            "Total Timesteps: 17813 Episode Num: 20 Reward: 194.74799296439744\n",
            "Total Timesteps: 18813 Episode Num: 21 Reward: 202.7327356779713\n",
            "Total Timesteps: 19813 Episode Num: 22 Reward: 91.99730705245665\n",
            "Total Timesteps: 20361 Episode Num: 23 Reward: 50.92544254033603\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 223.865121\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 21361 Episode Num: 24 Reward: 287.1233199600664\n",
            "Total Timesteps: 22361 Episode Num: 25 Reward: 107.55764737736055\n",
            "Total Timesteps: 23361 Episode Num: 26 Reward: 358.60545551451327\n",
            "Total Timesteps: 24361 Episode Num: 27 Reward: 260.2617764374819\n",
            "Total Timesteps: 25361 Episode Num: 28 Reward: 475.47414059221694\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 258.227377\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 26361 Episode Num: 29 Reward: 274.2613028776525\n",
            "Total Timesteps: 27361 Episode Num: 30 Reward: 371.77854692034197\n",
            "Total Timesteps: 28361 Episode Num: 31 Reward: 203.18209812378427\n",
            "Total Timesteps: 29361 Episode Num: 32 Reward: 202.99231901477947\n",
            "Total Timesteps: 30361 Episode Num: 33 Reward: 99.59194073136197\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 99.637950\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 31361 Episode Num: 34 Reward: 100.2682398011286\n",
            "Total Timesteps: 32361 Episode Num: 35 Reward: 206.18320312722813\n",
            "Total Timesteps: 33361 Episode Num: 36 Reward: 105.09979815769394\n",
            "Total Timesteps: 34361 Episode Num: 37 Reward: 112.93506453849471\n",
            "Total Timesteps: 35361 Episode Num: 38 Reward: 114.8263986780068\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 140.139361\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 36361 Episode Num: 39 Reward: 260.6553895632023\n",
            "Total Timesteps: 37361 Episode Num: 40 Reward: 106.60390414331556\n",
            "Total Timesteps: 38361 Episode Num: 41 Reward: 179.3486070326395\n",
            "Total Timesteps: 39361 Episode Num: 42 Reward: 152.8190712080697\n",
            "Total Timesteps: 40361 Episode Num: 43 Reward: 358.2733365493549\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 259.103068\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 41361 Episode Num: 44 Reward: 282.84779181737565\n",
            "Total Timesteps: 42361 Episode Num: 45 Reward: 353.46244881192115\n",
            "Total Timesteps: 43361 Episode Num: 46 Reward: 222.6605599943113\n",
            "Total Timesteps: 44361 Episode Num: 47 Reward: 253.20375910839795\n",
            "Total Timesteps: 45361 Episode Num: 48 Reward: 399.59977411654484\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 314.539987\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 46361 Episode Num: 49 Reward: 247.7682676673233\n",
            "Total Timesteps: 47361 Episode Num: 50 Reward: 154.532342246893\n",
            "Total Timesteps: 48361 Episode Num: 51 Reward: 328.8551855211624\n",
            "Total Timesteps: 49361 Episode Num: 52 Reward: 93.74704194891565\n",
            "Total Timesteps: 50361 Episode Num: 53 Reward: 185.39525280907952\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 387.482427\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 51361 Episode Num: 54 Reward: 405.55710470863056\n",
            "Total Timesteps: 52361 Episode Num: 55 Reward: 160.6242844900245\n",
            "Total Timesteps: 53361 Episode Num: 56 Reward: 360.4728816783443\n",
            "Total Timesteps: 54361 Episode Num: 57 Reward: 319.2120362076824\n",
            "Total Timesteps: 55361 Episode Num: 58 Reward: 418.3375873391474\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 431.858126\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 56361 Episode Num: 59 Reward: 339.86302761795463\n",
            "Total Timesteps: 57361 Episode Num: 60 Reward: 383.96406461042693\n",
            "Total Timesteps: 58361 Episode Num: 61 Reward: 253.7637888113957\n",
            "Total Timesteps: 59361 Episode Num: 62 Reward: 222.98497663203102\n",
            "Total Timesteps: 60361 Episode Num: 63 Reward: 368.2906984103045\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 379.001392\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 61361 Episode Num: 64 Reward: 460.83823568067226\n",
            "Total Timesteps: 62361 Episode Num: 65 Reward: 111.18642095038804\n",
            "Total Timesteps: 63361 Episode Num: 66 Reward: 102.9380887058839\n",
            "Total Timesteps: 64361 Episode Num: 67 Reward: 322.9594933756502\n",
            "Total Timesteps: 64396 Episode Num: 68 Reward: -2.3245693807722736\n",
            "Total Timesteps: 64466 Episode Num: 69 Reward: 7.3689858580229375\n",
            "Total Timesteps: 65466 Episode Num: 70 Reward: 125.99227679837597\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 386.835670\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 66466 Episode Num: 71 Reward: 327.51657310153297\n",
            "Total Timesteps: 67466 Episode Num: 72 Reward: 245.47641857270577\n",
            "Total Timesteps: 67487 Episode Num: 73 Reward: 0.20984245614520725\n",
            "Total Timesteps: 67513 Episode Num: 74 Reward: 1.1651931951376562\n",
            "Total Timesteps: 67534 Episode Num: 75 Reward: 0.7068235922376149\n",
            "Total Timesteps: 67555 Episode Num: 76 Reward: 0.6104104497109328\n",
            "Total Timesteps: 67590 Episode Num: 77 Reward: 5.959384659396142\n",
            "Total Timesteps: 67623 Episode Num: 78 Reward: 5.105920864398032\n",
            "Total Timesteps: 68623 Episode Num: 79 Reward: 336.020687592412\n",
            "Total Timesteps: 68801 Episode Num: 80 Reward: 98.38801903578673\n",
            "Total Timesteps: 68839 Episode Num: 81 Reward: 6.299463536123958\n",
            "Total Timesteps: 69839 Episode Num: 82 Reward: 235.12097356230615\n",
            "Total Timesteps: 70839 Episode Num: 83 Reward: 232.50504939911946\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 214.475255\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 71839 Episode Num: 84 Reward: 351.6067205707415\n",
            "Total Timesteps: 72839 Episode Num: 85 Reward: 265.23139129338546\n",
            "Total Timesteps: 73839 Episode Num: 86 Reward: 108.00642322329361\n",
            "Total Timesteps: 74839 Episode Num: 87 Reward: 195.09352845384524\n",
            "Total Timesteps: 75839 Episode Num: 88 Reward: 618.8761289204424\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 300.630327\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 76839 Episode Num: 89 Reward: 543.5482773795279\n",
            "Total Timesteps: 77839 Episode Num: 90 Reward: 521.4197064593736\n",
            "Total Timesteps: 78839 Episode Num: 91 Reward: 579.8720584022992\n",
            "Total Timesteps: 79349 Episode Num: 92 Reward: 297.8525064406475\n",
            "Total Timesteps: 80349 Episode Num: 93 Reward: 643.8302931882237\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 327.991826\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 81349 Episode Num: 94 Reward: 351.3004687637923\n",
            "Total Timesteps: 81610 Episode Num: 95 Reward: 135.7292467051756\n",
            "Total Timesteps: 82610 Episode Num: 96 Reward: 625.8714966216202\n",
            "Total Timesteps: 83610 Episode Num: 97 Reward: 487.8413998344328\n",
            "Total Timesteps: 84610 Episode Num: 98 Reward: 330.9203940760851\n",
            "Total Timesteps: 84734 Episode Num: 99 Reward: 6.758052036198562\n",
            "Total Timesteps: 84863 Episode Num: 100 Reward: 0.7117394062754501\n",
            "Total Timesteps: 84915 Episode Num: 101 Reward: -8.051261702203142\n",
            "Total Timesteps: 85915 Episode Num: 102 Reward: 189.46143923256588\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 3.402271\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 85992 Episode Num: 103 Reward: -5.539054296326812\n",
            "Total Timesteps: 86131 Episode Num: 104 Reward: 21.425572126396727\n",
            "Total Timesteps: 86158 Episode Num: 105 Reward: -7.911840573896294\n",
            "Total Timesteps: 86189 Episode Num: 106 Reward: -6.9202619097280955\n",
            "Total Timesteps: 87189 Episode Num: 107 Reward: 393.8344962060532\n",
            "Total Timesteps: 88189 Episode Num: 108 Reward: 282.7862755394797\n",
            "Total Timesteps: 89189 Episode Num: 109 Reward: 267.27625432121573\n",
            "Total Timesteps: 90189 Episode Num: 110 Reward: 164.34509209343895\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 228.953782\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 91189 Episode Num: 111 Reward: 229.06991280677664\n",
            "Total Timesteps: 91450 Episode Num: 112 Reward: 107.41018792696224\n",
            "Total Timesteps: 91710 Episode Num: 113 Reward: -36.81514270990595\n",
            "Total Timesteps: 92710 Episode Num: 114 Reward: 559.0974285051269\n",
            "Total Timesteps: 93710 Episode Num: 115 Reward: 135.90298600331968\n",
            "Total Timesteps: 94710 Episode Num: 116 Reward: 216.54611016150636\n",
            "Total Timesteps: 95710 Episode Num: 117 Reward: 188.92574789041612\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 196.129539\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 96710 Episode Num: 118 Reward: 145.94233790322903\n",
            "Total Timesteps: 97710 Episode Num: 119 Reward: 258.10061443308115\n",
            "Total Timesteps: 98710 Episode Num: 120 Reward: 338.98471964810454\n",
            "Total Timesteps: 99710 Episode Num: 121 Reward: 302.1430305087932\n",
            "Total Timesteps: 100710 Episode Num: 122 Reward: 238.50244959979648\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 453.878008\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 101710 Episode Num: 123 Reward: 540.9951701968854\n",
            "Total Timesteps: 102710 Episode Num: 124 Reward: 454.5869289598189\n",
            "Total Timesteps: 103710 Episode Num: 125 Reward: 271.10507221031924\n",
            "Total Timesteps: 104710 Episode Num: 126 Reward: 297.5592945057248\n",
            "Total Timesteps: 105710 Episode Num: 127 Reward: 224.9175545992713\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 529.195157\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 106710 Episode Num: 128 Reward: 695.1276400019688\n",
            "Total Timesteps: 107710 Episode Num: 129 Reward: 611.4204946640766\n",
            "Total Timesteps: 108710 Episode Num: 130 Reward: 362.4925513839044\n",
            "Total Timesteps: 109710 Episode Num: 131 Reward: 218.48390727021606\n",
            "Total Timesteps: 110710 Episode Num: 132 Reward: 400.3809211628767\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 317.530232\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 111710 Episode Num: 133 Reward: 350.0537005150049\n",
            "Total Timesteps: 112710 Episode Num: 134 Reward: 203.5752679230445\n",
            "Total Timesteps: 113710 Episode Num: 135 Reward: 649.5567584784532\n",
            "Total Timesteps: 114710 Episode Num: 136 Reward: 528.0027146422245\n",
            "Total Timesteps: 115710 Episode Num: 137 Reward: 621.484978979863\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 408.355693\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 116710 Episode Num: 138 Reward: 351.9070665024424\n",
            "Total Timesteps: 117710 Episode Num: 139 Reward: 397.1587978583933\n",
            "Total Timesteps: 118710 Episode Num: 140 Reward: 314.13647921586266\n",
            "Total Timesteps: 119710 Episode Num: 141 Reward: 499.30723616974205\n",
            "Total Timesteps: 120710 Episode Num: 142 Reward: 566.0623344801386\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 467.177005\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 121710 Episode Num: 143 Reward: 565.5411121186972\n",
            "Total Timesteps: 122710 Episode Num: 144 Reward: 304.70649326225345\n",
            "Total Timesteps: 123710 Episode Num: 145 Reward: 454.5978312527374\n",
            "Total Timesteps: 124710 Episode Num: 146 Reward: 407.78840478125824\n",
            "Total Timesteps: 125710 Episode Num: 147 Reward: 247.08980649138513\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 386.014534\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 126710 Episode Num: 148 Reward: 489.31794328831336\n",
            "Total Timesteps: 127710 Episode Num: 149 Reward: 385.99994232907596\n",
            "Total Timesteps: 128710 Episode Num: 150 Reward: 440.5771767450459\n",
            "Total Timesteps: 129710 Episode Num: 151 Reward: 429.9252474770403\n",
            "Total Timesteps: 130710 Episode Num: 152 Reward: 421.29196025932873\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 399.375172\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 131710 Episode Num: 153 Reward: 425.1248963857536\n",
            "Total Timesteps: 132710 Episode Num: 154 Reward: 403.1831064151811\n",
            "Total Timesteps: 133710 Episode Num: 155 Reward: 496.3796130784995\n",
            "Total Timesteps: 134710 Episode Num: 156 Reward: 484.09908650771774\n",
            "Total Timesteps: 135710 Episode Num: 157 Reward: 450.7059468494939\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 500.037688\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 136710 Episode Num: 158 Reward: 467.93545521189293\n",
            "Total Timesteps: 137710 Episode Num: 159 Reward: 625.2483587204047\n",
            "Total Timesteps: 138710 Episode Num: 160 Reward: 205.3829965915146\n",
            "Total Timesteps: 138979 Episode Num: 161 Reward: 70.73939511058059\n",
            "Total Timesteps: 139979 Episode Num: 162 Reward: 606.7228888630585\n",
            "Total Timesteps: 140979 Episode Num: 163 Reward: 508.30550735252365\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 469.080590\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 141979 Episode Num: 164 Reward: 356.52084958582526\n",
            "Total Timesteps: 142979 Episode Num: 165 Reward: 540.1253871051425\n",
            "Total Timesteps: 143979 Episode Num: 166 Reward: 506.7624322359314\n",
            "Total Timesteps: 144979 Episode Num: 167 Reward: 430.8281150884823\n",
            "Total Timesteps: 145979 Episode Num: 168 Reward: 413.19446872288626\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 410.371049\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 146979 Episode Num: 169 Reward: 450.29903208598273\n",
            "Total Timesteps: 147979 Episode Num: 170 Reward: 228.58868543048968\n",
            "Total Timesteps: 148979 Episode Num: 171 Reward: 160.91887605490038\n",
            "Total Timesteps: 149979 Episode Num: 172 Reward: 230.9606387609171\n",
            "Total Timesteps: 150979 Episode Num: 173 Reward: 415.6225156592219\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 364.123909\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 151979 Episode Num: 174 Reward: 302.61963163535086\n",
            "Total Timesteps: 152979 Episode Num: 175 Reward: 418.15841136772843\n",
            "Total Timesteps: 153979 Episode Num: 176 Reward: 402.5624627841699\n",
            "Total Timesteps: 154979 Episode Num: 177 Reward: 613.4936306834709\n",
            "Total Timesteps: 155979 Episode Num: 178 Reward: 333.27517985382093\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 332.409133\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 156979 Episode Num: 179 Reward: 387.05252913476545\n",
            "Total Timesteps: 157979 Episode Num: 180 Reward: 568.4330763034702\n",
            "Total Timesteps: 158979 Episode Num: 181 Reward: 501.04041276582996\n",
            "Total Timesteps: 159979 Episode Num: 182 Reward: 541.4294987056998\n",
            "Total Timesteps: 160979 Episode Num: 183 Reward: 636.7510605206285\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 458.413997\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 161979 Episode Num: 184 Reward: 377.375790705103\n",
            "Total Timesteps: 162979 Episode Num: 185 Reward: 499.51594368293604\n",
            "Total Timesteps: 163979 Episode Num: 186 Reward: 472.48047435961644\n",
            "Total Timesteps: 164979 Episode Num: 187 Reward: 565.3227069029851\n",
            "Total Timesteps: 165979 Episode Num: 188 Reward: 521.7001046695445\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 422.349877\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 166979 Episode Num: 189 Reward: 605.6522562544402\n",
            "Total Timesteps: 167098 Episode Num: 190 Reward: 88.01072212180193\n",
            "Total Timesteps: 167225 Episode Num: 191 Reward: 90.52542730345768\n",
            "Total Timesteps: 168225 Episode Num: 192 Reward: 268.38185759065453\n",
            "Total Timesteps: 169225 Episode Num: 193 Reward: 437.0032481497405\n",
            "Total Timesteps: 170225 Episode Num: 194 Reward: 549.2538777078593\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 457.282795\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 171225 Episode Num: 195 Reward: 261.26364815114357\n",
            "Total Timesteps: 172225 Episode Num: 196 Reward: 314.54427370565855\n",
            "Total Timesteps: 173225 Episode Num: 197 Reward: 444.8081868853122\n",
            "Total Timesteps: 174225 Episode Num: 198 Reward: 507.8414700364464\n",
            "Total Timesteps: 175225 Episode Num: 199 Reward: 439.6730417171668\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 502.737788\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 176225 Episode Num: 200 Reward: 417.2378520523878\n",
            "Total Timesteps: 177225 Episode Num: 201 Reward: 658.61939394175\n",
            "Total Timesteps: 178225 Episode Num: 202 Reward: 717.5059233927803\n",
            "Total Timesteps: 179225 Episode Num: 203 Reward: 624.9347402194692\n",
            "Total Timesteps: 180225 Episode Num: 204 Reward: 529.1162480966375\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 590.919250\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 181225 Episode Num: 205 Reward: 545.997570112981\n",
            "Total Timesteps: 182225 Episode Num: 206 Reward: 458.1049242819133\n",
            "Total Timesteps: 183225 Episode Num: 207 Reward: 496.3344252468023\n",
            "Total Timesteps: 184225 Episode Num: 208 Reward: 414.77338597055905\n",
            "Total Timesteps: 185225 Episode Num: 209 Reward: 443.14851512812106\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 582.829403\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 186225 Episode Num: 210 Reward: 606.7003547404896\n",
            "Total Timesteps: 187225 Episode Num: 211 Reward: 264.4037690610032\n",
            "Total Timesteps: 188225 Episode Num: 212 Reward: 625.894837726263\n",
            "Total Timesteps: 189225 Episode Num: 213 Reward: 594.7605208279141\n",
            "Total Timesteps: 190225 Episode Num: 214 Reward: 460.11626044373503\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 507.415137\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 191225 Episode Num: 215 Reward: 619.8838883904851\n",
            "Total Timesteps: 192225 Episode Num: 216 Reward: 419.00495645044595\n",
            "Total Timesteps: 193225 Episode Num: 217 Reward: 452.62915722677167\n",
            "Total Timesteps: 194225 Episode Num: 218 Reward: 321.23575448464726\n",
            "Total Timesteps: 195225 Episode Num: 219 Reward: 636.6042061864707\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 344.673447\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 196225 Episode Num: 220 Reward: 512.6412466150205\n",
            "Total Timesteps: 197225 Episode Num: 221 Reward: 641.4721891086749\n",
            "Total Timesteps: 198225 Episode Num: 222 Reward: 540.0686244242979\n",
            "Total Timesteps: 199225 Episode Num: 223 Reward: 559.7565654912307\n",
            "Total Timesteps: 200225 Episode Num: 224 Reward: 448.38565273870057\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 367.687850\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 201225 Episode Num: 225 Reward: 313.2346426994542\n",
            "Total Timesteps: 202225 Episode Num: 226 Reward: 512.2791178951079\n",
            "Total Timesteps: 202700 Episode Num: 227 Reward: 372.6045845140449\n",
            "Total Timesteps: 203700 Episode Num: 228 Reward: 563.2701933000675\n",
            "Total Timesteps: 204700 Episode Num: 229 Reward: 456.3174763115327\n",
            "Total Timesteps: 205700 Episode Num: 230 Reward: 334.16142686743694\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 603.726564\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 206700 Episode Num: 231 Reward: 637.9224477592394\n",
            "Total Timesteps: 207700 Episode Num: 232 Reward: 704.3984712056734\n",
            "Total Timesteps: 208700 Episode Num: 233 Reward: 533.843315543284\n",
            "Total Timesteps: 209700 Episode Num: 234 Reward: 539.7979541487966\n",
            "Total Timesteps: 210326 Episode Num: 235 Reward: 394.81005627490174\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 688.628877\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 211326 Episode Num: 236 Reward: 636.9156594571284\n",
            "Total Timesteps: 212326 Episode Num: 237 Reward: 695.7578182464022\n",
            "Total Timesteps: 213326 Episode Num: 238 Reward: 364.2698213339429\n",
            "Total Timesteps: 214326 Episode Num: 239 Reward: 772.6386552274339\n",
            "Total Timesteps: 215326 Episode Num: 240 Reward: 358.65733963757003\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 589.638188\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 216326 Episode Num: 241 Reward: 708.5178206154944\n",
            "Total Timesteps: 217326 Episode Num: 242 Reward: 649.3538130341432\n",
            "Total Timesteps: 218326 Episode Num: 243 Reward: 353.9038140651148\n",
            "Total Timesteps: 219326 Episode Num: 244 Reward: 608.2431896321319\n",
            "Total Timesteps: 220326 Episode Num: 245 Reward: 747.5802772464882\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 687.620071\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 221326 Episode Num: 246 Reward: 597.0115162884699\n",
            "Total Timesteps: 222326 Episode Num: 247 Reward: 754.879199823082\n",
            "Total Timesteps: 223326 Episode Num: 248 Reward: 675.8451387868002\n",
            "Total Timesteps: 224326 Episode Num: 249 Reward: 637.7966053781186\n",
            "Total Timesteps: 225326 Episode Num: 250 Reward: 653.6268094183982\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 589.350398\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 226326 Episode Num: 251 Reward: 719.6244196642466\n",
            "Total Timesteps: 227326 Episode Num: 252 Reward: 575.3254558868329\n",
            "Total Timesteps: 228326 Episode Num: 253 Reward: 788.2919910546982\n",
            "Total Timesteps: 229326 Episode Num: 254 Reward: 390.49662358328067\n",
            "Total Timesteps: 230326 Episode Num: 255 Reward: 484.72313352299403\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 596.308180\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 231326 Episode Num: 256 Reward: 480.28304985034424\n",
            "Total Timesteps: 232326 Episode Num: 257 Reward: 663.0663291154028\n",
            "Total Timesteps: 233326 Episode Num: 258 Reward: 672.8790126632057\n",
            "Total Timesteps: 234326 Episode Num: 259 Reward: 635.5275814227592\n",
            "Total Timesteps: 234646 Episode Num: 260 Reward: 222.81530533089196\n",
            "Total Timesteps: 235646 Episode Num: 261 Reward: 775.341176113367\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 654.876767\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 236646 Episode Num: 262 Reward: 554.1122312684105\n",
            "Total Timesteps: 237646 Episode Num: 263 Reward: 640.271399178652\n",
            "Total Timesteps: 238646 Episode Num: 264 Reward: 552.0091557788128\n",
            "Total Timesteps: 239646 Episode Num: 265 Reward: 768.6623046073414\n",
            "Total Timesteps: 240646 Episode Num: 266 Reward: 712.8893310997202\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 583.520839\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 241646 Episode Num: 267 Reward: 574.7727417176899\n",
            "Total Timesteps: 242646 Episode Num: 268 Reward: 546.8882234405704\n",
            "Total Timesteps: 243646 Episode Num: 269 Reward: 728.2816915780572\n",
            "Total Timesteps: 244646 Episode Num: 270 Reward: 645.341216158842\n",
            "Total Timesteps: 245646 Episode Num: 271 Reward: 589.0924964160662\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 605.274957\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 246646 Episode Num: 272 Reward: 595.6918503559361\n",
            "Total Timesteps: 247646 Episode Num: 273 Reward: 625.9943701875405\n",
            "Total Timesteps: 248646 Episode Num: 274 Reward: 635.4129737900753\n",
            "Total Timesteps: 249609 Episode Num: 275 Reward: 606.5164299122757\n",
            "Total Timesteps: 250609 Episode Num: 276 Reward: 569.8996786784598\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 627.005966\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 251609 Episode Num: 277 Reward: 582.5591752260971\n",
            "Total Timesteps: 252609 Episode Num: 278 Reward: 582.9449412549285\n",
            "Total Timesteps: 253609 Episode Num: 279 Reward: 570.1686348414551\n",
            "Total Timesteps: 254609 Episode Num: 280 Reward: 672.4868289173754\n",
            "Total Timesteps: 255609 Episode Num: 281 Reward: 579.4761199116768\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 731.906636\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 256609 Episode Num: 282 Reward: 774.5079171577603\n",
            "Total Timesteps: 257609 Episode Num: 283 Reward: 543.8620196933377\n",
            "Total Timesteps: 258609 Episode Num: 284 Reward: 562.6131397948682\n",
            "Total Timesteps: 259609 Episode Num: 285 Reward: 573.3217287518372\n",
            "Total Timesteps: 260609 Episode Num: 286 Reward: 445.7004176424273\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 607.145645\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 261609 Episode Num: 287 Reward: 552.3518290795638\n",
            "Total Timesteps: 262609 Episode Num: 288 Reward: 466.0858380499923\n",
            "Total Timesteps: 263609 Episode Num: 289 Reward: 691.4967763780365\n",
            "Total Timesteps: 264609 Episode Num: 290 Reward: 686.3674647909768\n",
            "Total Timesteps: 265609 Episode Num: 291 Reward: 646.0932715618817\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 600.705904\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 266609 Episode Num: 292 Reward: 613.6978043729556\n",
            "Total Timesteps: 267609 Episode Num: 293 Reward: 675.7856281353463\n",
            "Total Timesteps: 268609 Episode Num: 294 Reward: 638.3815354792453\n",
            "Total Timesteps: 269609 Episode Num: 295 Reward: 575.0829830003219\n",
            "Total Timesteps: 270609 Episode Num: 296 Reward: 762.3407977692137\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 579.508746\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 271609 Episode Num: 297 Reward: 566.707608997284\n",
            "Total Timesteps: 272609 Episode Num: 298 Reward: 558.3015788935444\n",
            "Total Timesteps: 273609 Episode Num: 299 Reward: 563.9756360234836\n",
            "Total Timesteps: 274609 Episode Num: 300 Reward: 661.5196824672738\n",
            "Total Timesteps: 275609 Episode Num: 301 Reward: 657.7537019134394\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 631.314524\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 276609 Episode Num: 302 Reward: 655.8865141516222\n",
            "Total Timesteps: 277609 Episode Num: 303 Reward: 474.52567103609374\n",
            "Total Timesteps: 278609 Episode Num: 304 Reward: 563.1554432074861\n",
            "Total Timesteps: 279609 Episode Num: 305 Reward: 406.85936485314085\n",
            "Total Timesteps: 280609 Episode Num: 306 Reward: 679.8004101235513\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 688.433083\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 281609 Episode Num: 307 Reward: 803.4295809052634\n",
            "Total Timesteps: 282609 Episode Num: 308 Reward: 782.7045313069157\n",
            "Total Timesteps: 283609 Episode Num: 309 Reward: 609.6665050701264\n",
            "Total Timesteps: 284609 Episode Num: 310 Reward: 635.118808309893\n",
            "Total Timesteps: 285609 Episode Num: 311 Reward: 778.1456807784772\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 623.305553\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 286609 Episode Num: 312 Reward: 532.1499372328674\n",
            "Total Timesteps: 287609 Episode Num: 313 Reward: 742.0358813530552\n",
            "Total Timesteps: 288609 Episode Num: 314 Reward: 777.3186339470243\n",
            "Total Timesteps: 289609 Episode Num: 315 Reward: 852.5436216462365\n",
            "Total Timesteps: 290609 Episode Num: 316 Reward: 725.6000511765999\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 692.135024\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 291609 Episode Num: 317 Reward: 518.7111042424015\n",
            "Total Timesteps: 292609 Episode Num: 318 Reward: 573.7594134568507\n",
            "Total Timesteps: 293609 Episode Num: 319 Reward: 661.7325146371015\n",
            "Total Timesteps: 294609 Episode Num: 320 Reward: 600.4364323191576\n",
            "Total Timesteps: 295609 Episode Num: 321 Reward: 699.2530114670022\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 692.824736\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 296609 Episode Num: 322 Reward: 702.9175261310966\n",
            "Total Timesteps: 297609 Episode Num: 323 Reward: 561.6665471099851\n",
            "Total Timesteps: 298609 Episode Num: 324 Reward: 724.8854661583207\n",
            "Total Timesteps: 299609 Episode Num: 325 Reward: 645.1755348917343\n",
            "Total Timesteps: 300609 Episode Num: 326 Reward: 771.2565853268252\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 734.679518\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 301026 Episode Num: 327 Reward: 254.6017010003755\n",
            "Total Timesteps: 302026 Episode Num: 328 Reward: 776.2920766458062\n",
            "Total Timesteps: 303026 Episode Num: 329 Reward: 782.3625728132604\n",
            "Total Timesteps: 304026 Episode Num: 330 Reward: 675.6898719592817\n",
            "Total Timesteps: 305026 Episode Num: 331 Reward: 795.1731423693562\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 718.711819\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 306026 Episode Num: 332 Reward: 790.6800101050469\n",
            "Total Timesteps: 307026 Episode Num: 333 Reward: 704.7818892697745\n",
            "Total Timesteps: 308026 Episode Num: 334 Reward: 769.3683432374966\n",
            "Total Timesteps: 309026 Episode Num: 335 Reward: 872.249564173847\n",
            "Total Timesteps: 310026 Episode Num: 336 Reward: 527.6191442860229\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 642.131973\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 311026 Episode Num: 337 Reward: 505.59242300151567\n",
            "Total Timesteps: 312026 Episode Num: 338 Reward: 798.1345733037998\n",
            "Total Timesteps: 313026 Episode Num: 339 Reward: 884.6606865406876\n",
            "Total Timesteps: 314026 Episode Num: 340 Reward: 729.7380069589077\n",
            "Total Timesteps: 315026 Episode Num: 341 Reward: 718.117125489366\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 714.062874\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 316026 Episode Num: 342 Reward: 642.8199800108603\n",
            "Total Timesteps: 317026 Episode Num: 343 Reward: 798.0463019048337\n",
            "Total Timesteps: 318026 Episode Num: 344 Reward: 663.8137671809827\n",
            "Total Timesteps: 319026 Episode Num: 345 Reward: 575.1671613920764\n",
            "Total Timesteps: 320026 Episode Num: 346 Reward: 761.6756721992136\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 697.282418\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 321026 Episode Num: 347 Reward: 735.4831382247556\n",
            "Total Timesteps: 322026 Episode Num: 348 Reward: 766.1783054238128\n",
            "Total Timesteps: 323026 Episode Num: 349 Reward: 621.2291964408748\n",
            "Total Timesteps: 324026 Episode Num: 350 Reward: 841.929649965533\n",
            "Total Timesteps: 325026 Episode Num: 351 Reward: 711.5298663999587\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 807.091141\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 326026 Episode Num: 352 Reward: 805.7612233479041\n",
            "Total Timesteps: 327026 Episode Num: 353 Reward: 739.9426811025357\n",
            "Total Timesteps: 328026 Episode Num: 354 Reward: 781.699735312268\n",
            "Total Timesteps: 329026 Episode Num: 355 Reward: 659.8696777915992\n",
            "Total Timesteps: 330026 Episode Num: 356 Reward: 821.1863427789837\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 701.641581\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 331026 Episode Num: 357 Reward: 797.3529680661925\n",
            "Total Timesteps: 332026 Episode Num: 358 Reward: 741.6915308153133\n",
            "Total Timesteps: 333026 Episode Num: 359 Reward: 611.0457451859975\n",
            "Total Timesteps: 334026 Episode Num: 360 Reward: 676.4077626954514\n",
            "Total Timesteps: 335026 Episode Num: 361 Reward: 510.44644541888687\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 792.141812\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 336026 Episode Num: 362 Reward: 692.6362783740988\n",
            "Total Timesteps: 337026 Episode Num: 363 Reward: 789.1281894833195\n",
            "Total Timesteps: 338026 Episode Num: 364 Reward: 755.7231407302585\n",
            "Total Timesteps: 339026 Episode Num: 365 Reward: 694.0770105660484\n",
            "Total Timesteps: 340026 Episode Num: 366 Reward: 787.3234372536884\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 828.185420\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 341026 Episode Num: 367 Reward: 826.6393185435193\n",
            "Total Timesteps: 342026 Episode Num: 368 Reward: 783.5995591666257\n",
            "Total Timesteps: 343026 Episode Num: 369 Reward: 935.9882597623553\n",
            "Total Timesteps: 344026 Episode Num: 370 Reward: 592.4964006258668\n",
            "Total Timesteps: 345026 Episode Num: 371 Reward: 904.1099933094481\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 862.135411\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 346026 Episode Num: 372 Reward: 949.9794358405796\n",
            "Total Timesteps: 347026 Episode Num: 373 Reward: 950.7252376884708\n",
            "Total Timesteps: 348026 Episode Num: 374 Reward: 585.72206376421\n",
            "Total Timesteps: 349026 Episode Num: 375 Reward: 691.9430924438084\n",
            "Total Timesteps: 350026 Episode Num: 376 Reward: 666.7935673942794\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1077.900701\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 351026 Episode Num: 377 Reward: 1084.932389268249\n",
            "Total Timesteps: 352026 Episode Num: 378 Reward: 737.7147104388374\n",
            "Total Timesteps: 353026 Episode Num: 379 Reward: 500.82041066428513\n",
            "Total Timesteps: 354026 Episode Num: 380 Reward: 1256.7339501134745\n",
            "Total Timesteps: 355026 Episode Num: 381 Reward: 964.7211496493511\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 779.025464\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 356026 Episode Num: 382 Reward: 699.6587696951543\n",
            "Total Timesteps: 357026 Episode Num: 383 Reward: 656.256234767488\n",
            "Total Timesteps: 358026 Episode Num: 384 Reward: 877.447066815984\n",
            "Total Timesteps: 359026 Episode Num: 385 Reward: 897.6440104378299\n",
            "Total Timesteps: 360026 Episode Num: 386 Reward: 851.1779221661714\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 793.677434\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 361026 Episode Num: 387 Reward: 745.0255084663024\n",
            "Total Timesteps: 362026 Episode Num: 388 Reward: 933.5980804102394\n",
            "Total Timesteps: 363026 Episode Num: 389 Reward: 1327.6276791148039\n",
            "Total Timesteps: 364026 Episode Num: 390 Reward: 601.430454494804\n",
            "Total Timesteps: 365026 Episode Num: 391 Reward: 1372.5910272301714\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1268.399727\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 366026 Episode Num: 392 Reward: 1351.562531380669\n",
            "Total Timesteps: 367026 Episode Num: 393 Reward: 760.3194596411557\n",
            "Total Timesteps: 368026 Episode Num: 394 Reward: 1138.8165705621154\n",
            "Total Timesteps: 369026 Episode Num: 395 Reward: 1210.8152152187145\n",
            "Total Timesteps: 370026 Episode Num: 396 Reward: 1230.0989147819598\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 885.000058\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 371026 Episode Num: 397 Reward: 1099.3908428936847\n",
            "Total Timesteps: 372026 Episode Num: 398 Reward: 985.5663416466514\n",
            "Total Timesteps: 373026 Episode Num: 399 Reward: 1037.9879253709348\n",
            "Total Timesteps: 374026 Episode Num: 400 Reward: 1329.334463155456\n",
            "Total Timesteps: 375026 Episode Num: 401 Reward: 1471.5077797167796\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1260.608470\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 376026 Episode Num: 402 Reward: 1290.3141477451209\n",
            "Total Timesteps: 377026 Episode Num: 403 Reward: 1044.6846107917663\n",
            "Total Timesteps: 378026 Episode Num: 404 Reward: 1315.1045995477439\n",
            "Total Timesteps: 379026 Episode Num: 405 Reward: 974.3642751662558\n",
            "Total Timesteps: 380026 Episode Num: 406 Reward: 634.2248779965632\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 837.314491\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 381026 Episode Num: 407 Reward: 997.5570550301323\n",
            "Total Timesteps: 382026 Episode Num: 408 Reward: 1057.9793823553932\n",
            "Total Timesteps: 383026 Episode Num: 409 Reward: 952.721764864949\n",
            "Total Timesteps: 384026 Episode Num: 410 Reward: 725.2963667865337\n",
            "Total Timesteps: 385026 Episode Num: 411 Reward: 558.975599096516\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 831.181071\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 386026 Episode Num: 412 Reward: 1002.2643470188656\n",
            "Total Timesteps: 387026 Episode Num: 413 Reward: 1537.0194086235497\n",
            "Total Timesteps: 388026 Episode Num: 414 Reward: 861.0134810652237\n",
            "Total Timesteps: 389026 Episode Num: 415 Reward: 1491.7865118126458\n",
            "Total Timesteps: 390026 Episode Num: 416 Reward: 1678.3773223685414\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1242.470078\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 391026 Episode Num: 417 Reward: 954.1948047064936\n",
            "Total Timesteps: 392026 Episode Num: 418 Reward: 1456.3194549746104\n",
            "Total Timesteps: 393026 Episode Num: 419 Reward: 1496.8508139907697\n",
            "Total Timesteps: 394026 Episode Num: 420 Reward: 1317.0021445362017\n",
            "Total Timesteps: 395026 Episode Num: 421 Reward: 1481.338685079788\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1437.012993\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 396026 Episode Num: 422 Reward: 1455.856757949832\n",
            "Total Timesteps: 397026 Episode Num: 423 Reward: 1451.265698105399\n",
            "Total Timesteps: 398026 Episode Num: 424 Reward: 1683.9150075785087\n",
            "Total Timesteps: 399026 Episode Num: 425 Reward: 1563.9289139833618\n",
            "Total Timesteps: 400026 Episode Num: 426 Reward: 1625.7397756468763\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1552.557296\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 401026 Episode Num: 427 Reward: 1491.3876199288882\n",
            "Total Timesteps: 402026 Episode Num: 428 Reward: 1660.1920692266597\n",
            "Total Timesteps: 403026 Episode Num: 429 Reward: 752.6705626064397\n",
            "Total Timesteps: 404026 Episode Num: 430 Reward: 1459.843183717368\n",
            "Total Timesteps: 405026 Episode Num: 431 Reward: 1483.6992779504776\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1504.223190\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 406026 Episode Num: 432 Reward: 1819.242508670299\n",
            "Total Timesteps: 407026 Episode Num: 433 Reward: 1636.2457251755818\n",
            "Total Timesteps: 408026 Episode Num: 434 Reward: 1835.9688350821045\n",
            "Total Timesteps: 409026 Episode Num: 435 Reward: 254.5442947123529\n",
            "Total Timesteps: 410026 Episode Num: 436 Reward: 524.5282470391012\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 430.715715\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 411026 Episode Num: 437 Reward: 418.23715251529336\n",
            "Total Timesteps: 412026 Episode Num: 438 Reward: 1632.9157056080733\n",
            "Total Timesteps: 413026 Episode Num: 439 Reward: 381.4265100614646\n",
            "Total Timesteps: 414026 Episode Num: 440 Reward: 274.16264223243803\n",
            "Total Timesteps: 415026 Episode Num: 441 Reward: 202.4554474167302\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 222.180507\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 416026 Episode Num: 442 Reward: 103.84678885310251\n",
            "Total Timesteps: 416169 Episode Num: 443 Reward: 8.548684728516129\n",
            "Total Timesteps: 416287 Episode Num: 444 Reward: -5.61716742987002\n",
            "Total Timesteps: 417287 Episode Num: 445 Reward: 323.31419735353677\n",
            "Total Timesteps: 418287 Episode Num: 446 Reward: 188.8159462822312\n",
            "Total Timesteps: 419287 Episode Num: 447 Reward: 282.9862811772948\n",
            "Total Timesteps: 420287 Episode Num: 448 Reward: 170.97951788010704\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: -32.092050\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 420370 Episode Num: 449 Reward: -51.84700338878947\n",
            "Total Timesteps: 420451 Episode Num: 450 Reward: -41.09277791874515\n",
            "Total Timesteps: 421451 Episode Num: 451 Reward: 234.76042780687686\n",
            "Total Timesteps: 422451 Episode Num: 452 Reward: 93.35913877739532\n",
            "Total Timesteps: 423451 Episode Num: 453 Reward: 157.04848464403995\n",
            "Total Timesteps: 424451 Episode Num: 454 Reward: 103.2338390354001\n",
            "Total Timesteps: 425451 Episode Num: 455 Reward: 194.0976760502485\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: -36.743545\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 425563 Episode Num: 456 Reward: -61.85894867001236\n",
            "Total Timesteps: 426563 Episode Num: 457 Reward: 192.83621319773144\n",
            "Total Timesteps: 427563 Episode Num: 458 Reward: 290.3188734860801\n",
            "Total Timesteps: 428563 Episode Num: 459 Reward: 242.06855181149558\n",
            "Total Timesteps: 429563 Episode Num: 460 Reward: 311.0894964332949\n",
            "Total Timesteps: 430563 Episode Num: 461 Reward: 317.14528826345213\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 226.730677\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 431563 Episode Num: 462 Reward: 277.28278038783935\n",
            "Total Timesteps: 432563 Episode Num: 463 Reward: 179.08572863252977\n",
            "Total Timesteps: 432651 Episode Num: 464 Reward: -52.73252730001766\n",
            "Total Timesteps: 433651 Episode Num: 465 Reward: 184.329275396439\n",
            "Total Timesteps: 433731 Episode Num: 466 Reward: -42.61882192908549\n",
            "Total Timesteps: 433816 Episode Num: 467 Reward: -40.90375997091682\n",
            "Total Timesteps: 433901 Episode Num: 468 Reward: -47.173502377325676\n",
            "Total Timesteps: 433985 Episode Num: 469 Reward: -40.480832165533926\n",
            "Total Timesteps: 434073 Episode Num: 470 Reward: -43.63871768297309\n",
            "Total Timesteps: 434187 Episode Num: 471 Reward: -46.65182208168713\n",
            "Total Timesteps: 434289 Episode Num: 472 Reward: -56.79430683056084\n",
            "Total Timesteps: 434376 Episode Num: 473 Reward: -52.93295819938289\n",
            "Total Timesteps: 434462 Episode Num: 474 Reward: -55.31188191430471\n",
            "Total Timesteps: 434557 Episode Num: 475 Reward: -45.84934760726038\n",
            "Total Timesteps: 434663 Episode Num: 476 Reward: -45.96411114833548\n",
            "Total Timesteps: 434784 Episode Num: 477 Reward: -46.975499402331394\n",
            "Total Timesteps: 434866 Episode Num: 478 Reward: -48.82501211682595\n",
            "Total Timesteps: 434945 Episode Num: 479 Reward: -51.393817955829\n",
            "Total Timesteps: 435033 Episode Num: 480 Reward: -49.816224518564354\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: -27.613110\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 435128 Episode Num: 481 Reward: -48.27270395629309\n",
            "Total Timesteps: 435226 Episode Num: 482 Reward: -57.60661268493792\n",
            "Total Timesteps: 435353 Episode Num: 483 Reward: -40.29622248568141\n",
            "Total Timesteps: 435451 Episode Num: 484 Reward: -53.377439387650476\n",
            "Total Timesteps: 435653 Episode Num: 485 Reward: -37.80463621833312\n",
            "Total Timesteps: 436653 Episode Num: 486 Reward: 196.28171981411873\n",
            "Total Timesteps: 436739 Episode Num: 487 Reward: -56.590478034142194\n",
            "Total Timesteps: 436813 Episode Num: 488 Reward: -49.91509669205213\n",
            "Total Timesteps: 436878 Episode Num: 489 Reward: -47.29046251565803\n",
            "Total Timesteps: 436962 Episode Num: 490 Reward: -50.42579595773302\n",
            "Total Timesteps: 437047 Episode Num: 491 Reward: -47.71666796407056\n",
            "Total Timesteps: 437140 Episode Num: 492 Reward: -44.59552732584046\n",
            "Total Timesteps: 437249 Episode Num: 493 Reward: -42.609827351935635\n",
            "Total Timesteps: 437340 Episode Num: 494 Reward: -42.17611634597194\n",
            "Total Timesteps: 437440 Episode Num: 495 Reward: -45.55469899961058\n",
            "Total Timesteps: 437555 Episode Num: 496 Reward: -34.40039499607635\n",
            "Total Timesteps: 437653 Episode Num: 497 Reward: -48.07441463787946\n",
            "Total Timesteps: 437756 Episode Num: 498 Reward: -52.64382166973468\n",
            "Total Timesteps: 437852 Episode Num: 499 Reward: -46.66511862940556\n",
            "Total Timesteps: 437937 Episode Num: 500 Reward: -55.307250468763755\n",
            "Total Timesteps: 438017 Episode Num: 501 Reward: -48.705421495217415\n",
            "Total Timesteps: 438141 Episode Num: 502 Reward: -40.4535058959577\n",
            "Total Timesteps: 438220 Episode Num: 503 Reward: -47.21396130674037\n",
            "Total Timesteps: 438329 Episode Num: 504 Reward: -47.688453855090806\n",
            "Total Timesteps: 438444 Episode Num: 505 Reward: -39.88686379310694\n",
            "Total Timesteps: 438548 Episode Num: 506 Reward: -45.4545022988375\n",
            "Total Timesteps: 438627 Episode Num: 507 Reward: -42.91150911820854\n",
            "Total Timesteps: 438729 Episode Num: 508 Reward: -38.12311231206336\n",
            "Total Timesteps: 438803 Episode Num: 509 Reward: -44.967841153334454\n",
            "Total Timesteps: 438879 Episode Num: 510 Reward: -51.452462138876086\n",
            "Total Timesteps: 439046 Episode Num: 511 Reward: -15.411288490688348\n",
            "Total Timesteps: 439139 Episode Num: 512 Reward: -45.684664897516576\n",
            "Total Timesteps: 439209 Episode Num: 513 Reward: -42.92953134953863\n",
            "Total Timesteps: 439349 Episode Num: 514 Reward: -34.96807399771793\n",
            "Total Timesteps: 439420 Episode Num: 515 Reward: -48.86132847990071\n",
            "Total Timesteps: 439493 Episode Num: 516 Reward: -52.38650452444768\n",
            "Total Timesteps: 439559 Episode Num: 517 Reward: -49.4379801799612\n",
            "Total Timesteps: 439660 Episode Num: 518 Reward: -35.65038654624982\n",
            "Total Timesteps: 439755 Episode Num: 519 Reward: -41.0379065296423\n",
            "Total Timesteps: 439888 Episode Num: 520 Reward: -48.83212166495791\n",
            "Total Timesteps: 439959 Episode Num: 521 Reward: -54.84059890037522\n",
            "Total Timesteps: 440045 Episode Num: 522 Reward: -62.797207623969456\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: -60.535108\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 440122 Episode Num: 523 Reward: -47.532614302085015\n",
            "Total Timesteps: 440194 Episode Num: 524 Reward: -62.91045989233513\n",
            "Total Timesteps: 440282 Episode Num: 525 Reward: -49.818505356963186\n",
            "Total Timesteps: 440358 Episode Num: 526 Reward: -62.54904417728451\n",
            "Total Timesteps: 440423 Episode Num: 527 Reward: -58.600680654148974\n",
            "Total Timesteps: 440502 Episode Num: 528 Reward: -60.16683540192385\n",
            "Total Timesteps: 440588 Episode Num: 529 Reward: -53.72340774338265\n",
            "Total Timesteps: 440663 Episode Num: 530 Reward: -49.52101709091148\n",
            "Total Timesteps: 440740 Episode Num: 531 Reward: -43.83624790321851\n",
            "Total Timesteps: 440822 Episode Num: 532 Reward: -66.78760611780324\n",
            "Total Timesteps: 440894 Episode Num: 533 Reward: -46.50532813314392\n",
            "Total Timesteps: 440967 Episode Num: 534 Reward: -44.78778252665782\n",
            "Total Timesteps: 441039 Episode Num: 535 Reward: -49.5645252507369\n",
            "Total Timesteps: 441113 Episode Num: 536 Reward: -53.82600122241036\n",
            "Total Timesteps: 441204 Episode Num: 537 Reward: -54.080694614687395\n",
            "Total Timesteps: 441285 Episode Num: 538 Reward: -70.63197178872399\n",
            "Total Timesteps: 442285 Episode Num: 539 Reward: 189.08962653970679\n",
            "Total Timesteps: 442374 Episode Num: 540 Reward: -56.361642345432294\n",
            "Total Timesteps: 442500 Episode Num: 541 Reward: -49.38769304678718\n",
            "Total Timesteps: 442568 Episode Num: 542 Reward: -47.663241351159044\n",
            "Total Timesteps: 442666 Episode Num: 543 Reward: -47.35404984257537\n",
            "Total Timesteps: 442747 Episode Num: 544 Reward: -56.7992012094665\n",
            "Total Timesteps: 443747 Episode Num: 545 Reward: 210.3418392819531\n",
            "Total Timesteps: 443819 Episode Num: 546 Reward: -60.33209967381767\n",
            "Total Timesteps: 443887 Episode Num: 547 Reward: -61.501888869134554\n",
            "Total Timesteps: 443965 Episode Num: 548 Reward: -57.93245496508502\n",
            "Total Timesteps: 444044 Episode Num: 549 Reward: -63.62853249204616\n",
            "Total Timesteps: 444115 Episode Num: 550 Reward: -61.61421804773328\n",
            "Total Timesteps: 444184 Episode Num: 551 Reward: -63.58664347128833\n",
            "Total Timesteps: 444269 Episode Num: 552 Reward: -64.44070450427431\n",
            "Total Timesteps: 444343 Episode Num: 553 Reward: -56.64554102107525\n",
            "Total Timesteps: 445343 Episode Num: 554 Reward: 109.89519500119096\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 225.604515\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 446343 Episode Num: 555 Reward: 243.31541663685854\n",
            "Total Timesteps: 447343 Episode Num: 556 Reward: 89.99974703236133\n",
            "Total Timesteps: 448343 Episode Num: 557 Reward: 288.7432354525124\n",
            "Total Timesteps: 449343 Episode Num: 558 Reward: 251.3034002000537\n",
            "Total Timesteps: 450343 Episode Num: 559 Reward: 239.28149821241092\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 280.228474\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 451343 Episode Num: 560 Reward: 197.49354637550292\n",
            "Total Timesteps: 452343 Episode Num: 561 Reward: 280.9289712190435\n",
            "Total Timesteps: 453343 Episode Num: 562 Reward: 214.6129249728194\n",
            "Total Timesteps: 454343 Episode Num: 563 Reward: 307.1882136340905\n",
            "Total Timesteps: 455343 Episode Num: 564 Reward: 287.3008297351557\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 284.979734\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 456343 Episode Num: 565 Reward: 274.346521608273\n",
            "Total Timesteps: 457343 Episode Num: 566 Reward: 310.0209870782296\n",
            "Total Timesteps: 458343 Episode Num: 567 Reward: 271.23642280341124\n",
            "Total Timesteps: 459343 Episode Num: 568 Reward: 281.27546988112516\n",
            "Total Timesteps: 460343 Episode Num: 569 Reward: 288.8640796131392\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 340.524921\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 461343 Episode Num: 570 Reward: 620.2906000296123\n",
            "Total Timesteps: 462343 Episode Num: 571 Reward: 202.2972551817461\n",
            "Total Timesteps: 462454 Episode Num: 572 Reward: 79.78993401537048\n",
            "Total Timesteps: 463454 Episode Num: 573 Reward: 228.3706533098336\n",
            "Total Timesteps: 464454 Episode Num: 574 Reward: 924.6704474662033\n",
            "Total Timesteps: 465454 Episode Num: 575 Reward: 784.0241880060092\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 927.434641\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 466454 Episode Num: 576 Reward: 655.9467910084782\n",
            "Total Timesteps: 467454 Episode Num: 577 Reward: 755.9683335419509\n",
            "Total Timesteps: 468454 Episode Num: 578 Reward: 1058.2014212178226\n",
            "Total Timesteps: 469454 Episode Num: 579 Reward: 917.0767999817269\n",
            "Total Timesteps: 470454 Episode Num: 580 Reward: 734.1317112650456\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 795.999552\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 471454 Episode Num: 581 Reward: 857.6793602097536\n",
            "Total Timesteps: 472454 Episode Num: 582 Reward: 436.8500461855325\n",
            "Total Timesteps: 473454 Episode Num: 583 Reward: 1326.3050425508127\n",
            "Total Timesteps: 474454 Episode Num: 584 Reward: 1345.9600820018723\n",
            "Total Timesteps: 475454 Episode Num: 585 Reward: 1469.8805252687255\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1332.383194\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 476454 Episode Num: 586 Reward: 1514.238599779266\n",
            "Total Timesteps: 477454 Episode Num: 587 Reward: 855.4878282935948\n",
            "Total Timesteps: 478454 Episode Num: 588 Reward: 1124.50335276239\n",
            "Total Timesteps: 479454 Episode Num: 589 Reward: 1614.36585942361\n",
            "Total Timesteps: 480454 Episode Num: 590 Reward: 1542.1751552415703\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1052.640368\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 481454 Episode Num: 591 Reward: 986.7153161472785\n",
            "Total Timesteps: 482454 Episode Num: 592 Reward: 921.6826803685818\n",
            "Total Timesteps: 483454 Episode Num: 593 Reward: 1040.2396623999296\n",
            "Total Timesteps: 484454 Episode Num: 594 Reward: 1155.0458868930425\n",
            "Total Timesteps: 485454 Episode Num: 595 Reward: 965.6728276946974\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1446.164958\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 486454 Episode Num: 596 Reward: 1501.3203845429732\n",
            "Total Timesteps: 487454 Episode Num: 597 Reward: 1446.7885841513928\n",
            "Total Timesteps: 488454 Episode Num: 598 Reward: 1534.4033882786068\n",
            "Total Timesteps: 489454 Episode Num: 599 Reward: 1548.9307624515382\n",
            "Total Timesteps: 490454 Episode Num: 600 Reward: 1666.6294552503205\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1514.272796\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 491454 Episode Num: 601 Reward: 1591.5554854699944\n",
            "Total Timesteps: 492454 Episode Num: 602 Reward: 1572.5400018429941\n",
            "Total Timesteps: 493454 Episode Num: 603 Reward: 1648.8567879237376\n",
            "Total Timesteps: 494454 Episode Num: 604 Reward: 1704.5919604074957\n",
            "Total Timesteps: 495454 Episode Num: 605 Reward: 1702.2494605733916\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1700.765302\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 496454 Episode Num: 606 Reward: 1720.9483545704202\n",
            "Total Timesteps: 497454 Episode Num: 607 Reward: 1826.0301816024244\n",
            "Total Timesteps: 498454 Episode Num: 608 Reward: 1834.4377809433417\n",
            "Total Timesteps: 499454 Episode Num: 609 Reward: 1646.7394619894837\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1732.771914\n",
            "-------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e"
      },
      "source": [
        "## Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "69c25196-8959-46a7-bb28-38f5b3beaaf9"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Definimos el primero de los Críticos como red neuronal profunda\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Definimos el segundo de los Críticos como red neuronal profunda\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Propagación hacia adelante del primero de los Críticos\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Propagación hacia adelante del segundo de los Críticos\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "  \n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selección del dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construir todo el proceso de entrenamiento en una clase\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clipping=0.5, policy_freq=2):\n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Paso 4: Tomamos una muestra de transiciones (s, s’, a, r) de la memoria.\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Paso 5: A partir del estado siguiente s', el Actor del Target ejecuta la siguiente acción a'.\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Paso 6: Añadimos ruido gaussiano a la siguiente acción a' y lo cortamos para tenerlo en el rango de valores aceptado por el entorno.\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device) \n",
        "      noise = noise.clamp(-noise_clipping, noise_clipping)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Paso 7: Los dos Críticos del Target toman un par (s’, a’) como entrada y devuelven dos Q-values Qt1(s’,a’) y Qt2(s’,a’) como salida.\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Paso 8: Nos quedamos con el mínimo de los dos Q-values: min(Qt1, Qt2). Representa el valor aproximado del estado siguiente.\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Paso 9: Obtenemos el target final de los dos Crítico del Modelo, que es: Qt = r + γ * min(Qt1, Qt2), donde γ es el factor de descuento.\n",
        "      target_Q = reward + ((1-done) * discount * target_Q).detach()\n",
        "\n",
        "      # Paso 10: Los dos Críticos del Modelo toman un par (s, a) como entrada y devuelven dos Q-values Q1(s,a) y Q2(s,a) como salida.\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Paso 11: Calculamos la pérdida procedente de los Crítico del Modelo: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Paso 12: Propagamos hacia atrás la pérdida del crítico y actualizamos los parámetros de los dos Crítico del Modelo con un SGD.\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Paso 13: Cada dos iteraciones, actualizamos nuestro modelo de Actor ejecutando el gradiente ascendente en la salida del primer modelo crítico.\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_optimizer.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Paso 14: Todavía cada dos iteraciones, actualizamos los pesos del Actor del Target usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "        # Paso 15: Todavía cada dos iteraciones, actualizamos los pesos del target del Crítico usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "  # Método para guardar el modelo entrenado\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n",
        "\n",
        "  # Método para cargar el modelo entrenado\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load(\"%s/%s_actor.pth\" % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load(\"%s/%s_critic.pth\" % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"------------------------------------------------\")\n",
        "  print (\"Recompensa promedio en el paso de Evaluación: %f\" % (avg_reward))\n",
        "  print (\"------------------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Configuración: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Configuración: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1750.844197\n",
            "------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}